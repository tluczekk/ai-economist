{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/multi_agent_training_with_rllib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "It is helpful to be familiar with **Foundation**, a multi-agent economic simulator built for the AI Economist ([paper here](https://arxiv.org/abs/2004.13332)). If you haven't worked with Foundation before, we highly recommend taking a look at our other tutorials:\n",
    "\n",
    "- [Foundation: the Basics](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)\n",
    "- [Extending Foundation](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)\n",
    "- [Optimal Taxation Theory and Simulation](https://github.com/salesforce/ai-economist/blob/master/tutorials/optimal_taxation_theory_and_simulation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! This tutorial is the first of a series on doing distributed multi-agent reinforcement learning (MARL). Here, we specifically demonstrate how to integrate our multi-agent economic simulation, [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation), with [RLlib](https://github.com/ray-project/ray/tree/master/rllib), an open-source library for reinforcement learning. We chose to use RLlib, as it provides an easy-to-use and flexible library for MARL. A detailed documentation on RLlib is available [here](https://docs.ray.io/en/master/rllib.html).\n",
    "\n",
    "We put together these tutorial notebook with the following key goals in mind:\n",
    "- Provide an exposition to MARL. While there are many libraries and references out there for single-agent RL training, MARL training is not discussed as much, and there aren't many multi-agent rl libraries.\n",
    "- Provide reference starting code to perform MARL training so the AI Economist community can focus more on building meaningful extensions to Foundation and better-performant algorithms.\n",
    "\n",
    "We will cover the following concepts in this tutorial:\n",
    "1. Adding an *environment wrapper* to make the economic simulation compatible with RLlib.\n",
    "2. Creating a *trainer* object that holds the (multi-agent) policies for environment interaction.\n",
    "3. Training all the agents in the economic simulation.\n",
    "4. Generate a rollout using the trainer object and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies:\n",
    "You can install the ai-economist package using \n",
    "- the pip package manager OR\n",
    "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal, sys, time\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/salesforce/ai-economist.git\n",
    "\n",
    "    %cd ai-economist\n",
    "    !pip install -e .\n",
    "    \n",
    "    # Restart the Python runtime to automatically use the installed packages\n",
    "    print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
    "    time.sleep(1)\n",
    "    os.kill(os.getpid(), signal.SIGKILL)\n",
    "else:\n",
    "    ! pip install ai-economist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install OpenAI Gym to help define the environment's observation and action spaces for use with RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `RLlib` reinforcement learning library:\n",
    "- First, install TensorFlow\n",
    "- Then, install ray[rllib]\n",
    "\n",
    "Note: RLlib natively supports TensorFlow (including TensorFlow Eager) as well as PyTorch, but most of its internals are framework agnostic. Here's a relevant [blogpost](https://medium.com/distributed-computing-with-ray/lessons-from-implementing-12-deep-rl-algorithms-in-tf-and-pytorch-1b412009297d) that compares running RLlib algorithms with TF and PyTorch. Overall, TF seems to run a bit faster than PyTorch, in our experience, and we will use that in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install these specific versions of tensorflow and rllib, that we used in our work.\n",
    "!pip install tensorflow==1.14\n",
    "!pip install \"ray[rllib]==0.8.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the tutorials folder\n",
    "import os, sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\"/content/ai-economist/tutorials\")\n",
    "else:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(\"__file__\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding an Environment Wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a configuration (introduced in [the basics tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basics.ipynb)) for the \"gather-trade-build\" environment with multiple mobile agents (that move, gather resources, build or trade) and a social planner that sets taxes according to (a scaled variant of) the 2018 US tax schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configuration (dictionary) for the \"gather-trade-build\" environment.\n",
    "\n",
    "env_config_dict = {\n",
    "    # ===== SCENARIO CLASS =====\n",
    "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
    "    # The environment object will be an instance of the Scenario class.\n",
    "    'scenario_name': 'layout_from_file/simple_wood_and_stone',\n",
    "    \n",
    "    # ===== COMPONENTS =====\n",
    "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
    "    #   \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
    "    #   {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
    "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
    "    'components': [\n",
    "        # (1) Building houses\n",
    "        ('Build', {\n",
    "            'skill_dist':                   'pareto', \n",
    "            'payment_max_skill_multiplier': 3,\n",
    "            'build_labor':                  10,\n",
    "            'payment':                      10\n",
    "        }),\n",
    "        # (2) Trading collectible resources\n",
    "        ('ContinuousDoubleAuction', {\n",
    "            'max_bid_ask':    10,\n",
    "            'order_labor':    0.25,\n",
    "            'max_num_orders': 5,\n",
    "            'order_duration': 50\n",
    "        }),\n",
    "        # (3) Movement and resource collection\n",
    "        ('Gather', {\n",
    "            'move_labor':    1,\n",
    "            'collect_labor': 1,\n",
    "            'skill_dist':    'pareto'\n",
    "        }),\n",
    "        # (4) Planner\n",
    "        ('PeriodicBracketTax', {\n",
    "            'period':          100,\n",
    "            'bracket_spacing': 'us-federal',\n",
    "            'usd_scaling':     1000,\n",
    "            'disable_taxes':   False\n",
    "        })\n",
    "    ],\n",
    "    \n",
    "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
    "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
    "    'env_layout_file': 'quadrant_25x25_20each_30clump.txt',\n",
    "    'starting_agent_coin': 10,\n",
    "    'fixed_four_skill_and_loc': True,\n",
    "    \n",
    "    # ===== STANDARD ARGUMENTS ======\n",
    "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
    "    'n_agents': 4,          # Number of non-planner agents (must be > 1)\n",
    "    'world_size': [25, 25], # [Height, Width] of the env world\n",
    "    'episode_length': 1000, # Number of timesteps per episode\n",
    "    \n",
    "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
    "    # Otherwise, the policy selects only 1 action.\n",
    "    'multi_action_mode_agents': False,\n",
    "    'multi_action_mode_planner': True,\n",
    "    \n",
    "    # When flattening observations, concatenate scalar & vector observations before output.\n",
    "    # Otherwise, return observations with minimal processing.\n",
    "    'flatten_observations': True,\n",
    "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
    "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
    "    'flatten_masks': True,\n",
    "    \n",
    "    # How often to save the dense logs\n",
    "    'dense_log_frequency': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we have seen in earlier [tutorials](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb), using `env = foundation.make_env_instance(**env_config)` creates an environment instance `env` with the specified configuration.\n",
    "\n",
    "In order to use this environment with RLlib, we will also need to add the environment's `observation_space` and `action_space` attributes. Additionally, the environment itself must subclass the [`MultiAgentEnv`](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) interface, which can return observations and rewards from multiple ready agents per step. To this end, we use an environment [wrapper](https://github.com/salesforce/ai-economist/blob/master/tutorials/rllib/env_wrapper.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EnvWrapper] Spaces\n",
      "[EnvWrapper] Obs (a)   \n",
      "action_mask    : (50,)\n",
      "flat           : (136,)\n",
      "time           : (1,)\n",
      "world-idx_map  : (2, 11, 11)\n",
      "world-map      : (7, 11, 11)\n",
      "\n",
      "\n",
      "[EnvWrapper] Obs (p)   \n",
      "action_mask    : (154,)\n",
      "flat           : (86,)\n",
      "p0             : (8,)\n",
      "p1             : (8,)\n",
      "p2             : (8,)\n",
      "p3             : (8,)\n",
      "time           : (1,)\n",
      "world-idx_map  : (2, 25, 25)\n",
      "world-map      : (6, 25, 25)\n",
      "\n",
      "\n",
      "[EnvWrapper] Action (a) Discrete(50)\n",
      "[EnvWrapper] Action (p) MultiDiscrete([22 22 22 22 22 22 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tluczekk/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:330: RuntimeWarning: invalid value encountered in cast\n",
      "  multiarray.copyto(a, fill_value, casting='unsafe')\n"
     ]
    }
   ],
   "source": [
    "from rllib.env_wrapper import RLlibEnvWrapper\n",
    "env_obj = RLlibEnvWrapper({\"env_config_dict\": env_config_dict}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon applying the wrapper to our environment, we have now defined observation and action spaces for the agents and the planner, indicated with `(a)` and `(p)` respectively. Also, (a useful tip) you can still access the environment instance and its attributes simply by using `env_obj.env`\n",
    "\n",
    "In summary, the observation spaces are represented as `Box` objects and the action spaces as `Discrete` objects (for more details on these types, see the OpenAI documentation [page](https://gym.openai.com/docs/#spaces)).\n",
    "\n",
    "Briefly looking at the shapes of the observation features (the numbers in parentheses), you will see that we have some one-dimensional features (e.g. `action-mask`, `flat`, `time`) as well as spatial features (e.g., `world-idx-map`, `world-map`)\n",
    "\n",
    "A couple of quick notes:\n",
    "- An `action_mask` is used to mask out the actions that are not allowed by the environment. For instance, a mobile agent cannot move beyond the boundary of the world. Hence, in position (0, 0), a mobile cannot move \"Left\" or \"Down\", and the corresponding actions in the mask would be nulled out. Now, the RL agent can still recommend to move \"Left\" or \"Down\", but the action isn't really taken.\n",
    "- The key `flat` arises since we set `flatten_observations': True`. Accordingly, the scalar and vector raw observations are all concatenated into this single key. If you're curious to see the entire set of raw observations, do set `flatten_observations': False` in the env_config, and re-run the above cell.\n",
    "\n",
    "Looking at the action spaces, the mobile agents can take 50 possible actions (including 1 NO-OP action or do nothing (always indexed 0), 44 trading-related actions, 4 move actions along the four directions and 1 build action)\n",
    "\n",
    "The planner sets the tax rates for 7 brackets, each from 0-100% in steps of 5%, so that's 21 values. Adding the NO-OP action brings the planner action space to `MultiDiscrete([22 22 22 22 22 22 22])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a *Trainer* Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our economic simulation environment with RLlib, you will need familiarity with one of the key classes: the [`Trainer`](https://docs.ray.io/en/master/rllib-training.html). The trainer object maintains the relationships that connect each agent in the environment to its corresponding trainable policy, and essentially helps in training, checkpointing policies and inferring actions. It helps to co-ordinate the workflow of collecting rollouts and optimizing the various policies via a reinforcement learning algorithm. Inherently, RLlib maintains a wide suite of [algorithms](https://docs.ray.io/en/master/rllib-algorithms.html) for multi-agent learning (which was another strong reason for us to consider using RLlib) - available options include SAC, PPO, PG, A2C, A3C, IMPALA, ES, DDPG, DQN, MARWIL, APEX, and APEX_DDPG. For the remainder of this tutorial, we will stick to using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) (PPO), an algorithm known to perform well generally.\n",
    "\n",
    "Every algorithm has a corresponding trainer object; in the context of PPO, we invoke the `PPOTrainer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPOTrainer can be instantiated with \n",
    "- `env`: an environment creator (i.e, RLlibEnvWrapper() in our case)\n",
    "- `config`: algorithm-specific configuration data for setting the various components of the RL training loop including the environment, rollout worker processes, training resources, degree of parallelism, framework used, and the policy exploration strategies.\n",
    "\n",
    "Note: There are several configuration settings related to policy architectures, rollout collection, minibatching, and other important hyperparameters, that need to be set carefully in order to train effectively. For the sake of the high-level exposition, we allow RLlib to use most of the the default settings. Check out the list of default [common configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-training.html#common-parameters) and default [PPO-specific configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-algorithms.html?highlight=PPO#proximal-policy-optimization-ppo). Custom environment configurations may be passed to environment creator via `config[\"env_config\"]`.\n",
    "\n",
    "RLlib also chooses default built-in [models](https://docs.ray.io/en/releases-0.8.4/rllib-models.html#built-in-models-and-preprocessors) for processing the observations. The models are picked based on a simple heuristic: a [vision](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet.py) network for observations that have shape of length larger than 2 (for example, (84 x 84 x 3)), and a [fully connected](https://github.com/ray-project/ray/blob/master/rllib/models/tf/fcnet.py) network for everything else. Custom models can be configured via the `config[\"policy\"][\"model\"]` key.\n",
    "\n",
    "In the context of multi-agent training, we will also need to set the multi-agent configuration:\n",
    "```python\n",
    "\"multiagent\": {\n",
    "        # Map of type MultiAgentPolicyConfigDict from policy ids to tuples\n",
    "        # of (policy_cls, obs_space, act_space, config). This defines the\n",
    "        # observation and action spaces of the policies and any extra config.\n",
    "        \"policies\": {},\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": None,\n",
    "        # Optional list of policies to train, or None for all policies.\n",
    "        \"policies_to_train\": None,\n",
    "    },\n",
    "```\n",
    "\n",
    "To this end, let's notate the agent policy id by `\"a\"` and the planner policy id by `\"p\"`. We can set `policies`, `policy_mapping_fun` and `policies_to_train` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {\n",
    "    \"a\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space,\n",
    "        env_obj.action_space,\n",
    "        {}  # define a custom agent policy configuration.\n",
    "    ),\n",
    "    \"p\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space_pl,\n",
    "        env_obj.action_space_pl,\n",
    "        {}  # define a custom planner policy configuration.\n",
    "    )\n",
    "}\n",
    "\n",
    "# In foundation, all the agents have integer ids and the social planner has an id of \"p\"\n",
    "policy_mapping_fun = lambda i: \"a\" if str(i).isdigit() else \"p\"\n",
    "\n",
    "policies_to_train = [\"a\", \"p\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a multiagent trainer config holding the trainable policies and their mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policies_to_train\": policies_to_train,\n",
    "        \"policy_mapping_fn\": policy_mapping_fun,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With distributed RL, architectures typically comprise several **roll-out** and **trainer** workers operating in tandem\n",
    "![](assets/distributed_rl_architecture.png)\n",
    "\n",
    "The roll-out workers repeatedly step through the environment to generate and collect roll-outs in parallel, using the actions sampled from the policy models on the roll-out workers or provided by the trainer worker.\n",
    "Roll-out workers typically use CPU machines, and sometimes, GPU machines for richer environments.\n",
    "Trainer workers gather the roll-out data (asynchronously) from the roll-out workers and optimize policies on CPU or GPU machines.\n",
    "\n",
    "In this context, we can also add a `num_workers` configuration parameter to specify the number of rollout workers, i.e, those responsible for gathering rollouts. Note: setting `num_workers=0` will mean the rollouts will be collected by the trainer worker itself. Also, each worker can collect rollouts from multiple environments in parallel, which is specified in `num_envs_per_worker`; there will be a total of `num_workers` $\\times$ `num_envs_per_worker` environment replicas used to gather rollouts.\n",
    "Note: below, we also update some of the default trainer settings to keep the iteration time small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_config.update(\n",
    "    {\n",
    "        \"num_workers\": 2,\n",
    "        \"num_envs_per_worker\": 2,\n",
    "        # Other training parameters\n",
    "        \"train_batch_size\":  4000,\n",
    "        \"sgd_minibatch_size\": 4000,\n",
    "        \"num_sgd_iter\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to add the environment configuration to the trainer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also add the \"num_envs_per_worker\" parameter for the env. wrapper to index the environments.\n",
    "env_config = {\n",
    "    \"env_config_dict\": env_config_dict,\n",
    "    \"num_envs_per_worker\": trainer_config.get('num_envs_per_worker'),   \n",
    "}\n",
    "\n",
    "trainer_config.update(\n",
    "    {\n",
    "        \"env_config\": env_config        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the training configuration is set, we will need to initialize ray and create the PPOTrainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 15:51:23,368\tINFO worker.py:1564 -- Connecting to existing Ray cluster at address: 172.22.13.239:6379...\n",
      "2024-05-23 15:51:23,375\tINFO worker.py:1749 -- Connected to Ray cluster.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbf2d8aeb90444b96cc3714b9f184f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.9.13</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.11.0</b></td>\n",
       "    </tr>\n",
       "    \n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.13', ray_version='2.11.0', ray_commit='2eb4a8119f903f79b78c01eaa9db06c6c390051c')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 15:51:24,492\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class 'rllib.env_wrapper.RLlibEnvWrapper'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'rllib.env_wrapper.RLlibEnvWrapper'>').build()` instead. This will raise an error in the future!\n",
      "/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:493: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(pid=12158)\u001b[0m /home/tluczekk/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "\u001b[36m(pid=12158)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=12159)\u001b[0m Inside covid19_components.py: 1 GPUs are available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m /home/tluczekk/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:330: RuntimeWarning: invalid value encountered in cast\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   multiarray.copyto(a, fill_value, casting='unsafe')\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m 2024-05-23 15:51:29,468\tWARNING env.py:298 -- Your MultiAgentEnv <RLlibEnvWrapper instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=12158, ip=172.22.13.239, actor_id=cc64fd720a7a81b38e5ff5c905000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f223dfc8040>)\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 535, in __init__\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1743, in _update_policy_map\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     self._build_policy_map(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1854, in _build_policy_map\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 141, in create_policy_for_framework\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 49, in __init__\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     TorchPolicyV2.__init__(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 94, in __init__\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     model, dist_class = self._init_model_and_dist_class()\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 512, in _init_model_and_dist_class\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\", line 831, in get_model_v2\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     return wrapper(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 79, in __init__\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     self.cnns[i] = ModelCatalog.get_model_v2(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\", line 831, in get_model_v2\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     return wrapper(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m   File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/utils.py\", line 291, in get_filter_config\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(RolloutWorker pid=12158)\u001b[0m ValueError: No default configuration for obs shape [2, 11, 11], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of the following shapes: [42, 42, K], [84, 84, K], [64, 64, K], [10, 10, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "2024-05-23 15:51:29,504\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=12158, ip=172.22.13.239, actor_id=cc64fd720a7a81b38e5ff5c905000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f223dfc8040>)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 535, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1743, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1854, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 141, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 49, in __init__\n",
      "    TorchPolicyV2.__init__(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 94, in __init__\n",
      "    model, dist_class = self._init_model_and_dist_class()\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 512, in _init_model_and_dist_class\n",
      "    model = ModelCatalog.get_model_v2(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\", line 831, in get_model_v2\n",
      "    return wrapper(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 79, in __init__\n",
      "    self.cnns[i] = ModelCatalog.get_model_v2(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\", line 831, in get_model_v2\n",
      "    return wrapper(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "    model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/utils.py\", line 291, in get_filter_config\n",
      "    raise ValueError(\n",
      "ValueError: No default configuration for obs shape [2, 11, 11], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of the following shapes: [42, 42, K], [84, 84, K], [64, 64, K], [10, 10, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "2024-05-23 15:51:29,506\tERROR actor_manager.py:517 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=12159, ip=172.22.13.239, actor_id=d6ed20acb32c0e0a07fbb4e705000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f276e1ee0a0>)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 535, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1743, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1854, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 141, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 49, in __init__\n",
      "    TorchPolicyV2.__init__(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 94, in __init__\n",
      "    model, dist_class = self._init_model_and_dist_class()\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 512, in _init_model_and_dist_class\n",
      "    model = ModelCatalog.get_model_v2(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\", line 831, in get_model_v2\n",
      "    return wrapper(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 79, in __init__\n",
      "    self.cnns[i] = ModelCatalog.get_model_v2(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\", line 831, in get_model_v2\n",
      "    return wrapper(\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "    model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "  File \"/home/tluczekk/anaconda3/lib/python3.9/site-packages/ray/rllib/models/utils.py\", line 291, in get_filter_config\n",
      "    raise ValueError(\n",
      "ValueError: No default configuration for obs shape [2, 11, 11], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of the following shapes: [42, 42, K], [84, 84, K], [64, 64, K], [10, 10, K]. You may alternatively want to use a custom model or preprocessor.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No default configuration for obs shape [2, 11, 11], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of the following shapes: [42, 42, K], [84, 84, K], [64, 64, K], [10, 10, K]. You may alternatively want to use a custom model or preprocessor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11306/2364669531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the PPO trainer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trainer = PPO(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRLlibEnvWrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m         }\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_logfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Create a set of env runner actors via a WorkerSet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         self.workers = WorkerSet(\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0;31m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;31m# to a config mismatch) thrown inside the actor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# In any other case, raise the RayActorError as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.execute_task.function_executor\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\u001b[0m in \u001b[0;36mactor_method_executor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    689\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ray_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;31m# Set method_name and method as attributes to the executor closure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\u001b[0m in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# If tracing feature flag is not on, perform a no-op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tracing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_ray_trace_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             tracer: _opentelemetry.trace.Tracer = _opentelemetry.trace.get_tracer(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# if RLModule API is enabled, marl_module_spec holds the specs of the RLModules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarl_module_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_policy_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Update Policy's view requirements from Model, only if Policy directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\u001b[0m in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# If tracing feature flag is not on, perform a no-op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tracing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_ray_trace_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             tracer: _opentelemetry.trace.Tracer = _opentelemetry.trace.get_tracer(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m_update_policy_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# Builds the self.policy_map dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         self._build_policy_map(\n\u001b[0m\u001b[1;32m   1744\u001b[0m             \u001b[0mpolicy_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdated_policy_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\u001b[0m in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# If tracing feature flag is not on, perform a no-op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tracing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_ray_trace_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             tracer: _opentelemetry.trace.Tracer = _opentelemetry.trace.get_tracer(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m_build_policy_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1852\u001b[0m             \u001b[0;31m# Create the actual policy object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m                 new_policy = create_policy_for_framework(\n\u001b[0m\u001b[1;32m   1855\u001b[0m                     \u001b[0mpolicy_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m                     policy_class=get_tf_eager_cls_if_necessary(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/policy.py\u001b[0m in \u001b[0;36mcreate_policy_for_framework\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# Non-tf: No graph, no session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mvalidate_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         TorchPolicyV2.__init__(\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdist_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_model_and_dist_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Create multi-GPU model towers, if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\u001b[0m in \u001b[0;36m_init_model_and_dist_class\u001b[0;34m()\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             )\n\u001b[0;32m--> 512\u001b[0;31m             model = ModelCatalog.get_model_v2(\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0mobs_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\u001b[0m in \u001b[0;36mget_model_v2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0;31m# Wrap in the requested interface.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_interface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m             return wrapper(\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/complex_input_net.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 }\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# if self.cnn_type == \"atari\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 self.cnns[i] = ModelCatalog.get_model_v2(\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/models/catalog.py\u001b[0m in \u001b[0;36mget_model_v2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0;31m# Wrap in the requested interface.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_interface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m             return wrapper(\n\u001b[0m\u001b[1;32m    832\u001b[0m                 \u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/models/torch/visionnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     ):\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv_filters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv_filters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filter_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         TorchModelV2.__init__(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/rllib/models/utils.py\u001b[0m in \u001b[0;36mget_filter_config\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilters_10x10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;34m\"No default configuration for obs shape {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34m\", you must specify `conv_filters` manually as a model option. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No default configuration for obs shape [2, 11, 11], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of the following shapes: [42, 42, K], [84, 84, K], [64, 64, K], [10, 10, K]. You may alternatively want to use a custom model or preprocessor."
     ]
    }
   ],
   "source": [
    "# Create the PPO trainer.\n",
    "#trainer = PPO(\n",
    "#    env=RLlibEnvWrapper,\n",
    "#    config=trainer_config,\n",
    "#    )\n",
    "config = PPOConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We are now ready to perform training by invoking `trainer.train()`; we call it for just a few number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_ITERS = 5\n",
    "for iteration in range(NUM_ITERS):\n",
    "    print(f'********** Iter : {iteration} **********')\n",
    "    result = trainer.train()\n",
    "    print(f'''episode_reward_mean: {result.get('episode_reward_mean')}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the results will be logged to a subdirectory of `~/ray_results`. This subdirectory will contain a file `params.json` which contains the hyperparameters, a file `result.json` which contains a training summary for each episode and a TensorBoard file that can be used to visualize training process with TensorBoard by running|\n",
    "```shell\n",
    "tensorboard --logdir ~/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate and Visualize the Environment's Dense Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any point during training, we would also want to inspect the environment's dense logs in order to deep-dive into the training results. Introduced in our [basic tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb#Visualize-using-dense-logging), dense logs are basically logs of each agent's states, actions and rewards at every point in time, along with a snapshot of the world state.\n",
    "\n",
    "There are two equivalent ways to fetch the environment's dense logs using the trainer object.\n",
    "\n",
    "a. Simply retrieve the dense log from the workers' environment objects\n",
    "\n",
    "b. Generate dense log(s) from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Simply retrieve the dense log from the workers' environment objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each rollout worker, it's straightforward to retrieve the dense logs using some of the function attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we fetch the dense logs for each rollout worker and environment within\n",
    "\n",
    "dense_logs = {}\n",
    "# Note: worker 0 is reserved for the trainer actor\n",
    "for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\n",
    "    for env_id in range(trainer_config[\"num_envs_per_worker\"]):\n",
    "        dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "        trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should have num_workers x num_envs_per_worker number of dense logs\n",
    "print(dense_logs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Generate a dense log from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also use the trainer object directly to play out an episode. The advantage of this approach is that we can re-sample the policy model any number of times and generate several rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=1\n",
    "):\n",
    "    dense_logs = {}\n",
    "    for idx in range(num_dense_logs):\n",
    "        # Set initial states\n",
    "        agent_states = {}\n",
    "        for agent_idx in range(env_obj.env.n_agents):\n",
    "            agent_states[str(agent_idx)] = trainer.get_policy(\"a\").get_initial_state()\n",
    "        planner_states = trainer.get_policy(\"p\").get_initial_state()   \n",
    "\n",
    "        # Play out the episode\n",
    "        obs, info = env_obj.reset(force_dense_logging=True)\n",
    "        for t in range(env_obj.env.episode_length):\n",
    "            actions = {}\n",
    "            for agent_idx in range(env_obj.env.n_agents):\n",
    "                # Use the trainer object directly to sample actions for each agent\n",
    "                actions[str(agent_idx)] = trainer.compute_action(\n",
    "                    obs[str(agent_idx)], \n",
    "                    agent_states[str(agent_idx)], \n",
    "                    policy_id=\"a\",\n",
    "                    full_fetch=False\n",
    "                )\n",
    "\n",
    "            # Action sampling for the planner\n",
    "            actions[\"p\"] = trainer.compute_action(\n",
    "                obs['p'], \n",
    "                planner_states, \n",
    "                policy_id='p',\n",
    "                full_fetch=False\n",
    "            )\n",
    "\n",
    "            obs, rew, done, trunc, info = env_obj.step(actions)        \n",
    "            if done['__all__']:\n",
    "                break\n",
    "        dense_logs[idx] = env_obj.env.dense_log\n",
    "    return dense_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_logs = generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the episode dense logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain the dense logs, we can use the plotting utilities we have created to examine the episode dense logs and visualize the the world state, agent-wise quantities, movement, and trading events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plotting  # plotting utilities for visualizing env. state\n",
    "\n",
    "dense_log_idx = 0\n",
    "plotting.breakdown(dense_logs[dense_log_idx]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray after use\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for now. See you in the [next](https://github.com/salesforce/ai-economist/blob/master/tutorials/two_level_curriculum_learning_with_rllib.md) tutorial :)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b139ae6237d52fdba9828784496281cf73a0aa758bee758129adc75b2e77b95"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('rllib-training': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
